import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

def load_real_data():
    """
    åŠ è½½æ‚¨çš„çœŸå®æ•°æ®
    """
    try:
        # è¯»å–çœŸå®æ•°æ®
        df = pd.read_csv('merged_data.csv')
        print(f"æˆåŠŸåŠ è½½æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"æ‚£è€…æ•°é‡: {df['PatientID'].nunique()}")
        
        # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['PatientID', 'Sweat CH (uM)', 'Sweat Rate (uL/min)', 
                        'Total cholesterol (mg/dL)', 'Age (18>)', 'Gender', 
                        'CALCULATED BMI', 'HgA1C']
        
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"è­¦å‘Š: ç¼ºå¤±ä»¥ä¸‹åˆ—: {missing_cols}")
        
        return df
        
    except FileNotFoundError:
        print("é”™è¯¯: æ‰¾ä¸åˆ° merged_data.csv æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

def clean_and_prepare_data(df):
    """
    æ¸…æ´—å’Œå‡†å¤‡æ•°æ®
    """
    # åˆ›å»ºå·¥ä½œå‰¯æœ¬
    data = df.copy()
    
    # é‡å‘½ååˆ—ä»¥ä¾¿äºä½¿ç”¨
    column_mapping = {
        'Sweat CH (uM)': 'SweatCH',
        'Sweat Rate (uL/min)': 'SweatRate', 
        'Total cholesterol (mg/dL)': 'BloodCH',
        'TG (mg/dL)': 'BloodTG',
        'Age (18>)': 'Age',
        'CALCULATED BMI': 'BMI',
        'HgA1C': 'HbA1c',
        'Blood Pressure H': 'BloodPressure_H',
        'Blood Pressure L': 'BloodPressure_L',
        'Fat%': 'FatPercent'
    }
    
    # åªé‡å‘½åå­˜åœ¨çš„åˆ—
    existing_mapping = {k: v for k, v in column_mapping.items() if k in data.columns}
    data = data.rename(columns=existing_mapping)
    
    # ç§»é™¤ç¼ºå¤±å…³é”®æ•°æ®çš„è¡Œ
    key_columns = ['SweatCH', 'SweatRate', 'BloodCH', 'PatientID', 'Age', 'Gender', 'BMI']
    available_key_cols = [col for col in key_columns if col in data.columns]
    
    print(f"æ¸…æ´—å‰æ•°æ®è¡Œæ•°: {len(data)}")
    data = data.dropna(subset=available_key_cols)
    print(f"æ¸…æ´—åæ•°æ®è¡Œæ•°: {len(data)}")
    
    # å¡«å……å…¶ä»–ç¼ºå¤±å€¼
    if 'HbA1c' in data.columns:
        data['HbA1c'] = data['HbA1c'].fillna(data['HbA1c'].median())
    if 'BloodPressure_H' in data.columns:
        data['BloodPressure_H'] = data['BloodPressure_H'].fillna(data['BloodPressure_H'].median())
    if 'FatPercent' in data.columns:
        data['FatPercent'] = data['FatPercent'].fillna(data['FatPercent'].median())
    
    return data

def calculate_real_confounding_strength(df):
    """
    åŸºäºçœŸå®æ•°æ®è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦
    """
    # å®šä¹‰å¯ç”¨çš„æ··æ‚å› ç´ 
    potential_confounders = {
        'BMI': 'BMI',
        'Age': 'Age', 
        'Gender': 'Gender',
        'HbA1c': 'HbA1c',
        'Blood Pressure': 'BloodPressure_H',
        'Fat%': 'FatPercent'
    }
    
    # æ£€æŸ¥å“ªäº›æ··æ‚å› ç´ å®é™…å¯ç”¨
    available_confounders = {}
    for name, col in potential_confounders.items():
        if col in df.columns and df[col].notna().sum() > 0:
            available_confounders[name] = col
    
    print(f"å¯ç”¨çš„æ··æ‚å› ç´ : {list(available_confounders.keys())}")
    
    # æ±—æ¶²å’Œè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©
    sweat_biomarkers = ['SweatCH', 'SweatRate']
    blood_biomarkers = ['BloodCH']
    
    # å¦‚æœæœ‰ç”˜æ²¹ä¸‰é…¯æ•°æ®ï¼Œä¹ŸåŒ…å«è¿›æ¥
    if 'BloodTG' in df.columns:
        blood_biomarkers.append('BloodTG')
    
    confounder_results = []
    
    for conf_name, conf_col in available_confounders.items():
        for sweat_bio in sweat_biomarkers:
            if sweat_bio not in df.columns:
                continue
                
            for blood_bio in blood_biomarkers:
                if blood_bio not in df.columns:
                    continue
                
                # è·å–æœ‰æ•ˆæ•°æ®
                valid_data = df[[conf_col, sweat_bio, blood_bio]].dropna()
                
                if len(valid_data) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                    continue
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                try:
                    corr_conf_sweat = valid_data[conf_col].corr(valid_data[sweat_bio])
                    corr_conf_blood = valid_data[conf_col].corr(valid_data[blood_bio])
                    
                    # æ··æ‚å¼ºåº¦ = |ç›¸å…³ç³»æ•°çš„ä¹˜ç§¯|
                    confounding_strength = abs(corr_conf_sweat * corr_conf_blood)
                    
                    confounder_results.append({
                        'Confounder': conf_name,
                        'SweatBiomarker': sweat_bio,
                        'BloodBiomarker': blood_bio,
                        'Corr_Confounder_Sweat': corr_conf_sweat,
                        'Corr_Confounder_Blood': corr_conf_blood,
                        'ConfoundingStrength': confounding_strength,
                        'SampleSize': len(valid_data)
                    })
                    
                except Exception as e:
                    print(f"è®¡ç®—ç›¸å…³æ€§æ—¶å‡ºé”™ ({conf_name}, {sweat_bio}, {blood_bio}): {e}")
                    continue
    
    return pd.DataFrame(confounder_results)

def calculate_real_causal_adjustment_benefit(df):
    """
    åŸºäºçœŸå®æ•°æ®è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š - ä½¿ç”¨LinearRegressionå’ŒRandomForestä¸¤ç§æ¨¡å‹
    """
    # ç¡®å®šå¯ç”¨çš„æ··æ‚å› ç´ 
    potential_confounders = ['BMI', 'Age', 'Gender', 'HbA1c', 'BloodPressure_H', 'FatPercent']
    available_confounders = [col for col in potential_confounders if col in df.columns]
    
    sweat_features = ['SweatCH', 'SweatRate']
    available_sweat = [col for col in sweat_features if col in df.columns]
    
    target = 'BloodCH'
    
    if target not in df.columns or len(available_sweat) == 0:
        print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç›®æ ‡å˜é‡æˆ–æ±—æ¶²ç‰¹å¾")
        return pd.DataFrame()
    
    benefits = []
    
    for conf in available_confounders:
        try:
            # è·å–æœ‰æ•ˆæ•°æ®
            required_cols = available_sweat + [target, conf]
            valid_data = df[required_cols].dropna()
            
            if len(valid_data) < 20:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå»ºæ¨¡
                print(f"è·³è¿‡ {conf}: æ•°æ®ä¸è¶³ (åªæœ‰ {len(valid_data)} ä¸ªæ ·æœ¬)")
                continue
            
            # å‡†å¤‡æ•°æ®
            X_simple = valid_data[available_sweat]
            y = valid_data[target]
            X_adjusted = valid_data[available_sweat + [conf]]
            
            # === çº¿æ€§å›å½’æ¨¡å‹ ===
            # æ¨¡å‹1: ä»…ä½¿ç”¨æ±—æ¶²æŒ‡æ ‡
            model_lr_simple = LinearRegression()
            model_lr_simple.fit(X_simple, y)
            pred_lr_simple = model_lr_simple.predict(X_simple)
            r2_lr_simple = r2_score(y, pred_lr_simple)
            rmse_lr_simple = np.sqrt(mean_squared_error(y, pred_lr_simple))
            
            # æ¨¡å‹2: æ±—æ¶²æŒ‡æ ‡ + å½“å‰æ··æ‚å› ç´ 
            model_lr_adjusted = LinearRegression()
            model_lr_adjusted.fit(X_adjusted, y)
            pred_lr_adjusted = model_lr_adjusted.predict(X_adjusted)
            r2_lr_adjusted = r2_score(y, pred_lr_adjusted)
            rmse_lr_adjusted = np.sqrt(mean_squared_error(y, pred_lr_adjusted))
            
            # === éšæœºæ£®æ—æ¨¡å‹ ===
            # æ¨¡å‹1: ä»…ä½¿ç”¨æ±—æ¶²æŒ‡æ ‡
            model_rf_simple = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5)
            model_rf_simple.fit(X_simple, y)
            pred_rf_simple = model_rf_simple.predict(X_simple)
            r2_rf_simple = r2_score(y, pred_rf_simple)
            rmse_rf_simple = np.sqrt(mean_squared_error(y, pred_rf_simple))
            
            # æ¨¡å‹2: æ±—æ¶²æŒ‡æ ‡ + å½“å‰æ··æ‚å› ç´ 
            model_rf_adjusted = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5)
            model_rf_adjusted.fit(X_adjusted, y)
            pred_rf_adjusted = model_rf_adjusted.predict(X_adjusted)
            r2_rf_adjusted = r2_score(y, pred_rf_adjusted)
            rmse_rf_adjusted = np.sqrt(mean_squared_error(y, pred_rf_adjusted))
            
            # è®¡ç®—æ”¹å–„ç¨‹åº¦
            lr_r2_improvement = (r2_lr_adjusted - r2_lr_simple) / max(r2_lr_simple, 0.001)
            rf_r2_improvement = (r2_rf_adjusted - r2_rf_simple) / max(r2_rf_simple, 0.001)
            
            lr_rmse_improvement = (rmse_lr_simple - rmse_lr_adjusted) / rmse_lr_simple
            rf_rmse_improvement = (rmse_rf_simple - rmse_rf_adjusted) / rmse_rf_simple
            
            benefits.append({
                'Confounder': conf,
                # Linear Regressionç»“æœ
                'LR_R2_Simple': r2_lr_simple,
                'LR_R2_Adjusted': r2_lr_adjusted,
                'LR_R2_Improvement': lr_r2_improvement,
                'LR_RMSE_Simple': rmse_lr_simple,
                'LR_RMSE_Adjusted': rmse_lr_adjusted,
                'LR_RMSE_Improvement': lr_rmse_improvement,
                # Random Forestç»“æœ
                'RF_R2_Simple': r2_rf_simple,
                'RF_R2_Adjusted': r2_rf_adjusted,
                'RF_R2_Improvement': rf_r2_improvement,
                'RF_RMSE_Simple': rmse_rf_simple,
                'RF_RMSE_Adjusted': rmse_rf_adjusted,
                'RF_RMSE_Improvement': rf_rmse_improvement,
                # é€šç”¨ä¿¡æ¯
                'AbsoluteImprovement_LR': r2_lr_adjusted - r2_lr_simple,
                'AbsoluteImprovement_RF': r2_rf_adjusted - r2_rf_simple,
                'SampleSize': len(valid_data)
            })
            
            print(f"{conf}:")
            print(f"  çº¿æ€§å›å½’: RÂ²ä» {r2_lr_simple:.3f} åˆ° {r2_lr_adjusted:.3f} (æ”¹å–„ {lr_r2_improvement:.1%})")
            print(f"  éšæœºæ£®æ—: RÂ²ä» {r2_rf_simple:.3f} åˆ° {r2_rf_adjusted:.3f} (æ”¹å–„ {rf_r2_improvement:.1%})")
            
        except Exception as e:
            print(f"è®¡ç®— {conf} çš„è°ƒæ•´æ”¶ç›Šæ—¶å‡ºé”™: {e}")
            continue
    
    return pd.DataFrame(benefits)

def create_real_figure1e_data(df):
    """
    åŸºäºçœŸå®æ•°æ®åˆ›å»ºFigure 1Eæ•°æ®
    """
    print("\n=== è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦ ===")
    confounder_analysis = calculate_real_confounding_strength(df)
    
    if confounder_analysis.empty:
        print("é”™è¯¯: æ— æ³•è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦")
        return None, None, None, None
    
    print(f"è®¡ç®—äº† {len(confounder_analysis)} ä¸ªæ··æ‚å› ç´ ç»„åˆ")
    
    print("\n=== è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š ===")
    adjustment_benefits = calculate_real_causal_adjustment_benefit(df)
    
    if adjustment_benefits.empty:
        print("é”™è¯¯: æ— æ³•è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š")
        return None, None, None, None
    
    # ä¸ºFigure 1Eå‡†å¤‡æ±‡æ€»æ•°æ®
    blood_ch_analysis = confounder_analysis[
        confounder_analysis['BloodBiomarker'] == 'BloodCH'
    ]
    
    if blood_ch_analysis.empty:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä¸è¡€æ¶²èƒ†å›ºé†‡ç›¸å…³çš„æ··æ‚åˆ†æ")
        blood_ch_analysis = confounder_analysis
    
    # æŒ‰æ··æ‚å› ç´ æ±‡æ€»
    conf_summary = blood_ch_analysis.groupby('Confounder').agg({
        'ConfoundingStrength': 'max',
        'Corr_Confounder_Sweat': 'mean',
        'Corr_Confounder_Blood': 'mean'
    }).reset_index()
    
    # åˆå¹¶è°ƒæ•´æ”¶ç›Šæ•°æ® - åˆ†åˆ«ä¸ºLRå’ŒRFåˆ›å»ºæ•°æ®
    figure1e_data_lr = conf_summary.merge(
        adjustment_benefits[['Confounder', 'LR_R2_Improvement', 'AbsoluteImprovement_LR']], 
        on='Confounder',
        how='left'
    )
    
    figure1e_data_rf = conf_summary.merge(
        adjustment_benefits[['Confounder', 'RF_R2_Improvement', 'AbsoluteImprovement_RF']], 
        on='Confounder',
        how='left'
    )
    
    # å¡«å……ç¼ºå¤±å€¼å¹¶é‡å‘½å
    figure1e_data_lr['LR_R2_Improvement'] = figure1e_data_lr['LR_R2_Improvement'].fillna(0)
    figure1e_data_lr['AbsoluteImprovement_LR'] = figure1e_data_lr['AbsoluteImprovement_LR'].fillna(0)
    
    figure1e_data_rf['RF_R2_Improvement'] = figure1e_data_rf['RF_R2_Improvement'].fillna(0)
    figure1e_data_rf['AbsoluteImprovement_RF'] = figure1e_data_rf['AbsoluteImprovement_RF'].fillna(0)
    
    # é‡å‘½ååˆ—
    figure1e_data_lr = figure1e_data_lr.rename(columns={
        'ConfoundingStrength': 'Confounding_Strength',
        'LR_R2_Improvement': 'Causal_Adjustment_Benefit',
        'AbsoluteImprovement_LR': 'Absolute_R2_Improvement'
    })
    
    figure1e_data_rf = figure1e_data_rf.rename(columns={
        'ConfoundingStrength': 'Confounding_Strength',
        'RF_R2_Improvement': 'Causal_Adjustment_Benefit',
        'AbsoluteImprovement_RF': 'Absolute_R2_Improvement'
    })
    
    # æŒ‰æ··æ‚å¼ºåº¦æ’åº
    figure1e_data_lr = figure1e_data_lr.sort_values('Confounding_Strength', ascending=False)
    figure1e_data_rf = figure1e_data_rf.sort_values('Confounding_Strength', ascending=False)
    
    return confounder_analysis, adjustment_benefits, figure1e_data_lr, figure1e_data_rf

def plot_comparison_figure1e_matplotlib(figure1e_data_lr, figure1e_data_rf, save_path='Figure1E_Comparison.png'):
    """
    åˆ›å»ºæ¯”è¾ƒLinearRegressionå’ŒRandomForestçš„Figure 1E
    """
    if figure1e_data_lr is None or figure1e_data_lr.empty:
        print("é”™è¯¯: æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜å›¾")
        return
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # === å·¦å›¾ï¼šLinear Regression ===
    x_pos = np.arange(len(figure1e_data_lr))
    confounders = figure1e_data_lr['Confounder'].values
    
    # æ··æ‚å¼ºåº¦ (å·¦Yè½´)
    color1 = '#FF9F40'
    bars1 = ax1.bar(x_pos - 0.2, figure1e_data_lr['Confounding_Strength'], 
                    width=0.4, label='Confounding Strength', 
                    color=color1, alpha=0.8, edgecolor='#FF6B35', linewidth=1.5)
    
    ax1.set_xlabel('Confounding Variables', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Confounding Strength', color=color1, fontsize=12, fontweight='bold')
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.set_ylim(0, max(figure1e_data_lr['Confounding_Strength']) * 1.3)
    
    # å› æœè°ƒæ•´æ”¶ç›Š (å³Yè½´)
    ax1_twin = ax1.twinx()
    color2 = '#4BC0C0'
    bars2 = ax1_twin.bar(x_pos + 0.2, figure1e_data_lr['Causal_Adjustment_Benefit'], 
                        width=0.4, label='Linear Regression Benefit',
                        color=color2, alpha=0.8, edgecolor='#36A2A2', linewidth=1.5)
    
    ax1_twin.set_ylabel('Prediction Improvement (LR)', color=color2, fontsize=12, fontweight='bold')
    ax1_twin.tick_params(axis='y', labelcolor=color2)
    ax1_twin.set_ylim(0, max(figure1e_data_lr['Causal_Adjustment_Benefit']) * 1.3)
    
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(confounders, rotation=45, ha='right', fontsize=10)
    ax1.set_title('Linear Regression Model\nConfounder Analysis', fontsize=14, fontweight='bold', pad=20)
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.set_facecolor('#fafafa')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
        height1 = bar1.get_height()
        height2 = bar2.get_height()
        
        ax1.annotate(f'{height1:.3f}',
                    xy=(bar1.get_x() + bar1.get_width() / 2, height1),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontsize=8, fontweight='bold')
        
        if height2 > 0:
            ax1_twin.annotate(f'+{height2:.1%}' if height2 < 1 else f'+{height2:.1f}x',
                            xy=(bar2.get_x() + bar2.get_width() / 2, height2),
                            xytext=(0, 3), textcoords="offset points",
                            ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # === å³å›¾ï¼šRandom Forest ===
    x_pos_rf = np.arange(len(figure1e_data_rf))
    confounders_rf = figure1e_data_rf['Confounder'].values
    
    # æ··æ‚å¼ºåº¦ (å·¦Yè½´)
    bars3 = ax2.bar(x_pos_rf - 0.2, figure1e_data_rf['Confounding_Strength'], 
                    width=0.4, label='Confounding Strength', 
                    color=color1, alpha=0.8, edgecolor='#FF6B35', linewidth=1.5)
    
    ax2.set_xlabel('Confounding Variables', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Confounding Strength', color=color1, fontsize=12, fontweight='bold')
    ax2.tick_params(axis='y', labelcolor=color1)
    ax2.set_ylim(0, max(figure1e_data_rf['Confounding_Strength']) * 1.3)
    
    # å› æœè°ƒæ•´æ”¶ç›Š (å³Yè½´)
    ax2_twin = ax2.twinx()
    color3 = '#9B59B6'  # ç´«è‰²ç”¨äºåŒºåˆ†Random Forest
    bars4 = ax2_twin.bar(x_pos_rf + 0.2, figure1e_data_rf['Causal_Adjustment_Benefit'], 
                        width=0.4, label='Random Forest Benefit',
                        color=color3, alpha=0.8, edgecolor='#8E44AD', linewidth=1.5)
    
    ax2_twin.set_ylabel('Prediction Improvement (RF)', color=color3, fontsize=12, fontweight='bold')
    ax2_twin.tick_params(axis='y', labelcolor=color3)
    ax2_twin.set_ylim(0, max(figure1e_data_rf['Causal_Adjustment_Benefit']) * 1.3)
    
    ax2.set_xticks(x_pos_rf)
    ax2.set_xticklabels(confounders_rf, rotation=45, ha='right', fontsize=10)
    ax2.set_title('Random Forest Model\nConfounder Analysis', fontsize=14, fontweight='bold', pad=20)
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.set_facecolor('#fafafa')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar3, bar4) in enumerate(zip(bars3, bars4)):
        height3 = bar3.get_height()
        height4 = bar4.get_height()
        
        ax2.annotate(f'{height3:.3f}',
                    xy=(bar3.get_x() + bar3.get_width() / 2, height3),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontsize=8, fontweight='bold')
        
        if height4 > 0:
            ax2_twin.annotate(f'+{height4:.1%}' if height4 < 1 else f'+{height4:.1f}x',
                            xy=(bar4.get_x() + bar4.get_width() / 2, height4),
                            xytext=(0, 3), textcoords="offset points",
                            ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # æ€»æ ‡é¢˜
    fig.suptitle('Confounder Analysis Comparison: Linear Regression vs Random Forest\n' + 
                'Based on Real Patient Data', fontsize=16, fontweight='bold', y=0.98)
    
    # å›¾ä¾‹
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax1_twin.get_legend_handles_labels()
    lines3, labels3 = ax2_twin.get_legend_handles_labels()
    
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', framealpha=0.9)
    ax2.legend(lines1 + lines3, labels1 + labels3, loc='upper right', framealpha=0.9)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def save_comprehensive_results(df, confounder_analysis, adjustment_benefits, 
                             figure1e_data_lr, figure1e_data_rf):
    """
    ä¿å­˜æ‰€æœ‰åˆ†æç»“æœï¼ŒåŒ…æ‹¬ä¸¤ç§æ¨¡å‹çš„å¯¹æ¯”
    """
    # ä¿å­˜æ¸…æ´—åçš„åŸå§‹æ•°æ®
    df.to_csv('real_patient_data_cleaned.csv', index=False)
    print("âœ… æ¸…æ´—åçš„çœŸå®æ‚£è€…æ•°æ®å·²ä¿å­˜è‡³: real_patient_data_cleaned.csv")
    
    # ä¿å­˜æ··æ‚å› ç´ è¯¦ç»†åˆ†æ
    if confounder_analysis is not None and not confounder_analysis.empty:
        confounder_analysis.to_csv('real_confounder_detailed_analysis.csv', index=False)
        print("âœ… æ··æ‚å› ç´ è¯¦ç»†åˆ†æå·²ä¿å­˜è‡³: real_confounder_detailed_analysis.csv")
    
    # ä¿å­˜å› æœè°ƒæ•´æ”¶ç›Š (åŒ…å«ä¸¤ç§æ¨¡å‹çš„ç»“æœ)
    if adjustment_benefits is not None and not adjustment_benefits.empty:
        adjustment_benefits.to_csv('real_causal_adjustment_benefits_comparison.csv', index=False)
        print("âœ… åŒæ¨¡å‹å› æœè°ƒæ•´æ”¶ç›Šå·²ä¿å­˜è‡³: real_causal_adjustment_benefits_comparison.csv")
    
    # ä¿å­˜Figure 1Eç»˜å›¾æ•°æ® - Linear Regressionç‰ˆæœ¬
    if figure1e_data_lr is not None and not figure1e_data_lr.empty:
        figure1e_data_lr.to_csv('Figure1E_LinearRegression_plot_data.csv', index=False)
        print("ğŸ“Š çº¿æ€§å›å½’Figure 1Eæ•°æ®å·²ä¿å­˜è‡³: Figure1E_LinearRegression_plot_data.csv")
    
    # ä¿å­˜Figure 1Eç»˜å›¾æ•°æ® - Random Forestç‰ˆæœ¬
    if figure1e_data_rf is not None and not figure1e_data_rf.empty:
        figure1e_data_rf.to_csv('Figure1E_RandomForest_plot_data.csv', index=False)
        print("ğŸŒ² éšæœºæ£®æ—Figure 1Eæ•°æ®å·²ä¿å­˜è‡³: Figure1E_RandomForest_plot_data.csv")
    
    # åˆ›å»ºæ¨¡å‹å¯¹æ¯”æ±‡æ€»
    if (figure1e_data_lr is not None and not figure1e_data_lr.empty and 
        figure1e_data_rf is not None and not figure1e_data_rf.empty):
        
        comparison_data = figure1e_data_lr[['Confounder', 'Confounding_Strength']].copy()
        comparison_data['LR_Benefit'] = figure1e_data_lr['Causal_Adjustment_Benefit']
        comparison_data['RF_Benefit'] = figure1e_data_rf['Causal_Adjustment_Benefit']
        comparison_data['Benefit_Difference'] = comparison_data['RF_Benefit'] - comparison_data['LR_Benefit']
        comparison_data['Better_Model'] = comparison_data['Benefit_Difference'].apply(
            lambda x: 'Random Forest' if x > 0.01 else ('Linear Regression' if x < -0.01 else 'Similar')
        )
        
        comparison_data.to_csv('Model_Comparison_Summary.csv', index=False)
        print("âš–ï¸  æ¨¡å‹å¯¹æ¯”æ±‡æ€»å·²ä¿å­˜è‡³: Model_Comparison_Summary.csv")
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœé¢„è§ˆ
        print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯”é¢„è§ˆ ===")
        print(comparison_data.round(4))
    
    # åˆ›å»ºè¯¦ç»†è¯´æ˜æ–‡ä»¶
   # description = f"""
# åŒæ¨¡å‹å› æœæ¨æ–­åˆ†æç»“æœè¯´æ˜

## åˆ†ææ¦‚è§ˆ
# - åŸå§‹æ•°æ®: {len(df)} ä¸ªæ ·æœ¬, {df['PatientID'].nunique()} åæ‚£è€…
# - åˆ†ææ¨¡å‹: Linear Regression vs Random Forest Regressor
# - åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}

## æ–‡ä»¶è¯´æ˜

### ğŸ“Š ç»˜å›¾æ•°æ®æ–‡ä»¶
# 1. **Figure1E_LinearRegression_plot_data.csv** - çº¿æ€§å›å½’æ¨¡å‹ç»“æœ
# 2. **Figure1E_RandomForest_plot_data.csv** - éšæœºæ£®æ—æ¨¡å‹ç»“æœ
# 3. **Model_Comparison_Summary.csv** - ä¸¤æ¨¡å‹ç›´æ¥å¯¹æ¯”

### ğŸ“ˆ è¯¦ç»†åˆ†ææ–‡ä»¶
# 4. **real_causal_adjustment_benefits_comparison.csv** - åŒ…å«ä¸¤ç§æ¨¡å‹å®Œæ•´ç»“æœ
# 5. **real_confounder_detailed_analysis.csv** - æ··æ‚å› ç´ è¯¦ç»†åˆ†æ
# 6. **real_patient_data_cleaned.csv** - æ¸…æ´—åçš„åŸå§‹æ•°æ®

## ä¸»è¦å‘ç°

### æ··import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def load_real_data():
    """
    åŠ è½½æ‚¨çš„çœŸå®æ•°æ®
    """
    try:
        # è¯»å–çœŸå®æ•°æ®
        df = pd.read_csv('merged_data.csv')
        print(f"æˆåŠŸåŠ è½½æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"æ‚£è€…æ•°é‡: {df['PatientID'].nunique()}")
        
        # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['PatientID', 'Sweat CH (uM)', 'Sweat Rate (uL/min)', 
                        'Total cholesterol (mg/dL)', 'Age (18>)', 'Gender', 
                        'CALCULATED BMI', 'HgA1C']
        
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"è­¦å‘Š: ç¼ºå¤±ä»¥ä¸‹åˆ—: {missing_cols}")
        
        return df
        
    except FileNotFoundError:
        print("é”™è¯¯: æ‰¾ä¸åˆ° merged_data.csv æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

def clean_and_prepare_data(df):
    """
    æ¸…æ´—å’Œå‡†å¤‡æ•°æ®
    """
    # åˆ›å»ºå·¥ä½œå‰¯æœ¬
    data = df.copy()
    
    # é‡å‘½ååˆ—ä»¥ä¾¿äºä½¿ç”¨
    column_mapping = {
        'Sweat CH (uM)': 'SweatCH',
        'Sweat Rate (uL/min)': 'SweatRate', 
        'Total cholesterol (mg/dL)': 'BloodCH',
        'TG (mg/dL)': 'BloodTG',
        'Age (18>)': 'Age',
        'CALCULATED BMI': 'BMI',
        'HgA1C': 'HbA1c',
        'Blood Pressure H': 'BloodPressure_H',
        'Blood Pressure L': 'BloodPressure_L',
        'Fat%': 'FatPercent'
    }
    
    # åªé‡å‘½åå­˜åœ¨çš„åˆ—
    existing_mapping = {k: v for k, v in column_mapping.items() if k in data.columns}
    data = data.rename(columns=existing_mapping)
    
    # ç§»é™¤ç¼ºå¤±å…³é”®æ•°æ®çš„è¡Œ
    key_columns = ['SweatCH', 'SweatRate', 'BloodCH', 'PatientID', 'Age', 'Gender', 'BMI']
    available_key_cols = [col for col in key_columns if col in data.columns]
    
    print(f"æ¸…æ´—å‰æ•°æ®è¡Œæ•°: {len(data)}")
    data = data.dropna(subset=available_key_cols)
    print(f"æ¸…æ´—åæ•°æ®è¡Œæ•°: {len(data)}")
    
    # å¡«å……å…¶ä»–ç¼ºå¤±å€¼
    if 'HbA1c' in data.columns:
        data['HbA1c'] = data['HbA1c'].fillna(data['HbA1c'].median())
    if 'BloodPressure_H' in data.columns:
        data['BloodPressure_H'] = data['BloodPressure_H'].fillna(data['BloodPressure_H'].median())
    if 'FatPercent' in data.columns:
        data['FatPercent'] = data['FatPercent'].fillna(data['FatPercent'].median())
    
    return data

def calculate_real_confounding_strength(df):
    """
    åŸºäºçœŸå®æ•°æ®è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦
    """
    # å®šä¹‰å¯ç”¨çš„æ··æ‚å› ç´ 
    potential_confounders = {
        'BMI': 'BMI',
        'Age': 'Age', 
        'Gender': 'Gender',
        'HbA1c': 'HbA1c',
        'Blood Pressure': 'BloodPressure_H',
        'Fat%': 'FatPercent'
    }
    
    # æ£€æŸ¥å“ªäº›æ··æ‚å› ç´ å®é™…å¯ç”¨
    available_confounders = {}
    for name, col in potential_confounders.items():
        if col in df.columns and df[col].notna().sum() > 0:
            available_confounders[name] = col
    
    print(f"å¯ç”¨çš„æ··æ‚å› ç´ : {list(available_confounders.keys())}")
    
    # æ±—æ¶²å’Œè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©
    sweat_biomarkers = ['SweatCH', 'SweatRate']
    blood_biomarkers = ['BloodCH']
    
    # å¦‚æœæœ‰ç”˜æ²¹ä¸‰é…¯æ•°æ®ï¼Œä¹ŸåŒ…å«è¿›æ¥
    if 'BloodTG' in df.columns:
        blood_biomarkers.append('BloodTG')
    
    confounder_results = []
    
    for conf_name, conf_col in available_confounders.items():
        for sweat_bio in sweat_biomarkers:
            if sweat_bio not in df.columns:
                continue
                
            for blood_bio in blood_biomarkers:
                if blood_bio not in df.columns:
                    continue
                
                # è·å–æœ‰æ•ˆæ•°æ®
                valid_data = df[[conf_col, sweat_bio, blood_bio]].dropna()
                
                if len(valid_data) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                    continue
                
                # è®¡ç®—ç›¸å…³ç³»æ•°
                try:
                    corr_conf_sweat = valid_data[conf_col].corr(valid_data[sweat_bio])
                    corr_conf_blood = valid_data[conf_col].corr(valid_data[blood_bio])
                    
                    # æ··æ‚å¼ºåº¦ = |ç›¸å…³ç³»æ•°çš„ä¹˜ç§¯|
                    confounding_strength = abs(corr_conf_sweat * corr_conf_blood)
                    
                    confounder_results.append({
                        'Confounder': conf_name,
                        'SweatBiomarker': sweat_bio,
                        'BloodBiomarker': blood_bio,
                        'Corr_Confounder_Sweat': corr_conf_sweat,
                        'Corr_Confounder_Blood': corr_conf_blood,
                        'ConfoundingStrength': confounding_strength,
                        'SampleSize': len(valid_data)
                    })
                    
                except Exception as e:
                    print(f"è®¡ç®—ç›¸å…³æ€§æ—¶å‡ºé”™ ({conf_name}, {sweat_bio}, {blood_bio}): {e}")
                    continue
    
    return pd.DataFrame(confounder_results)

def calculate_real_causal_adjustment_benefit(df):
    """
    åŸºäºçœŸå®æ•°æ®è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š
    """
    # ç¡®å®šå¯ç”¨çš„æ··æ‚å› ç´ 
    potential_confounders = ['BMI', 'Age', 'Gender', 'HbA1c', 'BloodPressure_H', 'FatPercent']
    available_confounders = [col for col in potential_confounders if col in df.columns]
    
    sweat_features = ['SweatCH', 'SweatRate']
    available_sweat = [col for col in sweat_features if col in df.columns]
    
    target = 'BloodCH'
    
    if target not in df.columns or len(available_sweat) == 0:
        print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç›®æ ‡å˜é‡æˆ–æ±—æ¶²ç‰¹å¾")
        return pd.DataFrame()
    
    benefits = []
    
    for conf in available_confounders:
        try:
            # è·å–æœ‰æ•ˆæ•°æ®
            required_cols = available_sweat + [target, conf]
            valid_data = df[required_cols].dropna()
            
            if len(valid_data) < 20:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå»ºæ¨¡
                print(f"è·³è¿‡ {conf}: æ•°æ®ä¸è¶³ (åªæœ‰ {len(valid_data)} ä¸ªæ ·æœ¬)")
                continue
            
            # æ¨¡å‹1: ä»…ä½¿ç”¨æ±—æ¶²æŒ‡æ ‡ 
            X_simple = valid_data[available_sweat]
            y = valid_data[target]
            
            model_simple = LinearRegression()
            model_simple.fit(X_simple, y)
            pred_simple = model_simple.predict(X_simple)
            r2_simple = r2_score(y, pred_simple)
            
            # æ¨¡å‹2: æ±—æ¶²æŒ‡æ ‡ + å½“å‰æ··æ‚å› ç´ 
            X_adjusted = valid_data[available_sweat + [conf]]
            
            model_adjusted = LinearRegression()
            model_adjusted.fit(X_adjusted, y)
            pred_adjusted = model_adjusted.predict(X_adjusted)
            r2_adjusted = r2_score(y, pred_adjusted)
            
            # è®¡ç®—æ”¹å–„ç¨‹åº¦
            absolute_improvement = r2_adjusted - r2_simple
            relative_improvement = absolute_improvement / max(r2_simple, 0.001)
            
            benefits.append({
                'Confounder': conf,
                'R2_Simple': r2_simple,
                'R2_Adjusted': r2_adjusted,
                'R2_Improvement': relative_improvement,
                'AbsoluteImprovement': absolute_improvement,
                'SampleSize': len(valid_data)
            })
            
            print(f"{conf}: RÂ²ä» {r2_simple:.3f} æå‡åˆ° {r2_adjusted:.3f} (æ”¹å–„ {relative_improvement:.1%})")
            
        except Exception as e:
            print(f"è®¡ç®— {conf} çš„è°ƒæ•´æ”¶ç›Šæ—¶å‡ºé”™: {e}")
            continue
    
    return pd.DataFrame(benefits)

def create_real_figure1e_data(df):
    """
    åŸºäºçœŸå®æ•°æ®åˆ›å»ºFigure 1Eæ•°æ®
    """
    print("\n=== è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦ ===")
    confounder_analysis = calculate_real_confounding_strength(df)
    
    if confounder_analysis.empty:
        print("é”™è¯¯: æ— æ³•è®¡ç®—æ··æ‚å› ç´ å¼ºåº¦")
        return None, None, None
    
    print(f"è®¡ç®—äº† {len(confounder_analysis)} ä¸ªæ··æ‚å› ç´ ç»„åˆ")
    
    print("\n=== è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š ===")
    adjustment_benefits = calculate_real_causal_adjustment_benefit(df)
    
    if adjustment_benefits.empty:
        print("é”™è¯¯: æ— æ³•è®¡ç®—å› æœè°ƒæ•´æ”¶ç›Š")
        return None, None, None
    
    # ä¸ºFigure 1Eå‡†å¤‡æ±‡æ€»æ•°æ®
    # å¯¹äºæ¯ä¸ªæ··æ‚å› ç´ ï¼Œå–ä¸è¡€æ¶²èƒ†å›ºé†‡ç›¸å…³çš„æœ€å¤§æ··æ‚å¼ºåº¦
    blood_ch_analysis = confounder_analysis[
        confounder_analysis['BloodBiomarker'] == 'BloodCH'
    ]
    
    if blood_ch_analysis.empty:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä¸è¡€æ¶²èƒ†å›ºé†‡ç›¸å…³çš„æ··æ‚åˆ†æ")
        # ä½¿ç”¨æ‰€æœ‰æ•°æ®
        blood_ch_analysis = confounder_analysis
    
    # æŒ‰æ··æ‚å› ç´ æ±‡æ€»
    conf_summary = blood_ch_analysis.groupby('Confounder').agg({
        'ConfoundingStrength': 'max',  # å–æœ€å¤§æ··æ‚å¼ºåº¦
        'Corr_Confounder_Sweat': 'mean',  # å¹³å‡ç›¸å…³æ€§
        'Corr_Confounder_Blood': 'mean'
    }).reset_index()
    
    # åˆå¹¶è°ƒæ•´æ”¶ç›Šæ•°æ®
    figure1e_data = conf_summary.merge(
        adjustment_benefits[['Confounder', 'R2_Improvement', 'AbsoluteImprovement']], 
        on='Confounder',
        how='left'
    )
    
    # å¡«å……ç¼ºå¤±çš„è°ƒæ•´æ”¶ç›Šæ•°æ®
    figure1e_data['R2_Improvement'] = figure1e_data['R2_Improvement'].fillna(0)
    figure1e_data['AbsoluteImprovement'] = figure1e_data['AbsoluteImprovement'].fillna(0)
    
    # é‡å‘½ååˆ—
    figure1e_data = figure1e_data.rename(columns={
        'ConfoundingStrength': 'Confounding_Strength',
        'R2_Improvement': 'Causal_Adjustment_Benefit',
        'AbsoluteImprovement': 'Absolute_R2_Improvement'
    })
    
    # æŒ‰æ··æ‚å¼ºåº¦æ’åº
    figure1e_data = figure1e_data.sort_values('Confounding_Strength', ascending=False)
    
    return confounder_analysis, adjustment_benefits, figure1e_data

def plot_real_figure1e_matplotlib(figure1e_data, save_path='Real_Figure1E_matplotlib.png'):
    """
    ä½¿ç”¨çœŸå®æ•°æ®åˆ›å»ºFigure 1E (matplotlibç‰ˆæœ¬)
    """
    if figure1e_data is None or figure1e_data.empty:
        print("é”™è¯¯: æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜å›¾")
        return
    
    fig, ax1 = plt.subplots(figsize=(12, 8))
    
    x_pos = np.arange(len(figure1e_data))
    confounders = figure1e_data['Confounder'].values
    
    # å·¦è½´ï¼šæ··æ‚å¼ºåº¦
    color1 = '#FF9F40'
    bars1 = ax1.bar(x_pos - 0.2, figure1e_data['Confounding_Strength'], 
                    width=0.4, label='Confounding Strength', 
                    color=color1, alpha=0.8, edgecolor='#FF6B35', linewidth=1.5)
    
    ax1.set_xlabel('Confounding Variables', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Confounding Strength', color=color1, fontsize=12, fontweight='bold')
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.set_ylim(0, max(figure1e_data['Confounding_Strength']) * 1.3)
    
    # å³è½´ï¼šå› æœè°ƒæ•´æ”¶ç›Š
    ax2 = ax1.twinx()
    color2 = '#4BC0C0'
    bars2 = ax2.bar(x_pos + 0.2, figure1e_data['Causal_Adjustment_Benefit'], 
                    width=0.4, label='Causal Adjustment Benefit',
                    color=color2, alpha=0.8, edgecolor='#36A2A2', linewidth=1.5)
    
    ax2.set_ylabel('Prediction Improvement (Fold Change)', color=color2, fontsize=12, fontweight='bold')
    ax2.tick_params(axis='y', labelcolor=color2)
    ax2.set_ylim(0, max(figure1e_data['Causal_Adjustment_Benefit']) * 1.3)
    
    # è®¾ç½®xè½´
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(confounders, rotation=45, ha='right', fontsize=11)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
        height1 = bar1.get_height()
        height2 = bar2.get_height()
        
        # æ··æ‚å¼ºåº¦æ ‡ç­¾
        ax1.annotate(f'{height1:.3f}',
                    xy=(bar1.get_x() + bar1.get_width() / 2, height1),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        # è°ƒæ•´æ”¶ç›Šæ ‡ç­¾
        if height2 > 0:
            ax2.annotate(f'+{height2:.1%}' if height2 < 1 else f'+{height2:.1f}x',
                        xy=(bar2.get_x() + bar2.get_width() / 2, height2),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # å›¾è¡¨æ ‡é¢˜
    plt.title('Confounder Analysis & Causal Adjustment Effects\n' + 
              'Based on Real Patient Data (n=115 samples, 23 patients)', 
              fontsize=14, fontweight='bold', pad=20)
    
    # å›¾ä¾‹
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', framealpha=0.9)
    
    # ç¾åŒ–
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.set_facecolor('#fafafa')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"Figure 1E (matplotlib) å·²ä¿å­˜è‡³: {save_path}")

def plot_real_figure1e_plotly(figure1e_data, save_path='Real_Figure1E_plotly.html'):
    """
    ä½¿ç”¨çœŸå®æ•°æ®åˆ›å»ºäº¤äº’å¼Figure 1E (plotlyç‰ˆæœ¬)
    """
    if figure1e_data is None or figure1e_data.empty:
        print("é”™è¯¯: æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜å›¾")
        return
    
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # æ··æ‚å¼ºåº¦æŸ±çŠ¶å›¾
    fig.add_trace(
        go.Bar(
            x=figure1e_data['Confounder'],
            y=figure1e_data['Confounding_Strength'],
            name='Confounding Strength',
            marker=dict(
                color='rgba(255, 159, 64, 0.8)',
                line=dict(color='#FF6B35', width=2)
            ),
            text=[f'{val:.3f}' for val in figure1e_data['Confounding_Strength']],
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>' +
                         'Confounding Strength: %{y:.3f}<br>' +
                         'Corr with Sweat: %{customdata[0]:.3f}<br>' +
                         'Corr with Blood: %{customdata[1]:.3f}<extra></extra>',
            customdata=np.column_stack((
                figure1e_data['Corr_Confounder_Sweat'],
                figure1e_data['Corr_Confounder_Blood']
            ))
        ),
        secondary_y=False,
    )
    
    # å› æœè°ƒæ•´æ”¶ç›ŠæŸ±çŠ¶å›¾
    fig.add_trace(
        go.Bar(
            x=figure1e_data['Confounder'],
            y=figure1e_data['Causal_Adjustment_Benefit'],
            name='Causal Adjustment Benefit',
            marker=dict(
                color='rgba(75, 192, 192, 0.8)',
                line=dict(color='#4BC0C0', width=2)
            ),
            text=[f'+{val:.1%}' if val < 1 else f'+{val:.1f}x' 
                  for val in figure1e_data['Causal_Adjustment_Benefit']],
            textposition='outside',
            hovertemplate='<b>%{x}</b><br>' +
                         'RÂ² Improvement: %{y:.1%}<br>' +
                         'Absolute Improvement: %{customdata:.3f}<extra></extra>',
            customdata=figure1e_data['Absolute_R2_Improvement']
        ),
        secondary_y=True,
    )
    
    # æ›´æ–°å¸ƒå±€
    fig.update_xaxes(
        title_text="Confounding Variables", 
        tickangle=-45,
        title_font=dict(size=14, color='black')
    )
    
    fig.update_yaxes(
        title_text="Confounding Strength", 
        secondary_y=False,
        title_font=dict(size=14, color='#FF6B35')
    )
    
    fig.update_yaxes(
        title_text="Prediction Improvement", 
        secondary_y=True,
        title_font=dict(size=14, color='#4BC0C0')
    )
    
    fig.update_layout(
        title={
            'text': 'Confounder Analysis & Causal Adjustment Effects<br>' +
                   '<sub>Based on Real Patient Data (n=115 samples, 23 patients)</sub>',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 16, 'color': 'black'}
        },
        barmode='group',
        bargap=0.15,
        bargroupgap=0.1,
        legend=dict(x=0.02, y=0.98, bgcolor='rgba(255,255,255,0.8)'),
        plot_bgcolor='#fafafa',
        paper_bgcolor='white',
        height=600,
        width=1000,
        margin=dict(t=100, b=100, l=80, r=80)
    )
    
    fig.write_html(save_path)
    fig.show()
    
    print(f"Figure 1E (plotly) å·²ä¿å­˜è‡³: {save_path}")

def save_real_data_csv(df, confounder_analysis, adjustment_benefits, figure1e_data):
    """
    ä¿å­˜åŸºäºçœŸå®æ•°æ®çš„æ‰€æœ‰åˆ†æç»“æœ
    """
    # ä¿å­˜æ¸…æ´—åçš„åŸå§‹æ•°æ®
    df.to_csv('real_patient_data_cleaned.csv', index=False)
    print("æ¸…æ´—åçš„çœŸå®æ‚£è€…æ•°æ®å·²ä¿å­˜è‡³: real_patient_data_cleaned.csv")
    
    # ä¿å­˜æ··æ‚å› ç´ è¯¦ç»†åˆ†æ
    if confounder_analysis is not None and not confounder_analysis.empty:
        confounder_analysis.to_csv('real_confounder_detailed_analysis.csv', index=False)
        print("çœŸå®æ•°æ®æ··æ‚å› ç´ è¯¦ç»†åˆ†æå·²ä¿å­˜è‡³: real_confounder_detailed_analysis.csv")
    
    # ä¿å­˜å› æœè°ƒæ•´æ”¶ç›Š
    if adjustment_benefits is not None and not adjustment_benefits.empty:
        adjustment_benefits.to_csv('real_causal_adjustment_benefits.csv', index=False)
        print("çœŸå®æ•°æ®å› æœè°ƒæ•´æ”¶ç›Šå·²ä¿å­˜è‡³: real_causal_adjustment_benefits.csv")
    
    # ä¿å­˜Figure 1Eç»˜å›¾æ•°æ® - è¿™æ˜¯æœ€é‡è¦çš„æ–‡ä»¶
    if figure1e_data is not None and not figure1e_data.empty:
        figure1e_data.to_csv('Real_Figure1E_plot_data.csv', index=False)
        print("â­ çœŸå®æ•°æ®Figure 1Eç»˜å›¾æ•°æ®å·²ä¿å­˜è‡³: Real_Figure1E_plot_data.csv")
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        print("\n=== Figure 1E ç»˜å›¾æ•°æ®é¢„è§ˆ ===")
        print(figure1e_data.round(4))
    
    # åˆ›å»ºæ•°æ®è¯´æ˜æ–‡ä»¶
    data_description = f"""
# åŸºäºçœŸå®æ•°æ®çš„ Figure 1E æ–‡ä»¶è¯´æ˜

## æ•°æ®æ¥æº
- åŸå§‹æ•°æ®: merged_data.csv ({len(df)} ä¸ªæ ·æœ¬, {df['PatientID'].nunique()} åæ‚£è€…)
- åˆ†ææ—¥æœŸ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}

## å…³é”®æ–‡ä»¶è¯´æ˜

### 1. Real_Figure1E_plot_data.csv â­â­â­ æœ€é‡è¦
**ç›´æ¥ç”¨äºç»˜åˆ¶Figure 1Eçš„æ•°æ®**
åˆ—è¯´æ˜ï¼š
- Confounder: æ··æ‚å› ç´ åç§° (xè½´)
- Confounding_Strength: æ··æ‚å¼ºåº¦ (å·¦yè½´ï¼Œæ©™è‰²æŸ±çŠ¶å›¾)
- Causal_Adjustment_Benefit: å› æœè°ƒæ•´æ”¶ç›Š (å³yè½´ï¼Œè“ç»¿è‰²æŸ±çŠ¶å›¾)
- Corr_Confounder_Sweat: æ··æ‚å› ç´ ä¸æ±—æ¶²æŒ‡æ ‡çš„ç›¸å…³æ€§
- Corr_Confounder_Blood: æ··æ‚å› ç´ ä¸è¡€æ¶²æŒ‡æ ‡çš„ç›¸å…³æ€§
- Absolute_R2_Improvement: RÂ²çš„ç»å¯¹æ”¹å–„å€¼

### 2. real_patient_data_cleaned.csv
æ¸…æ´—åçš„æ‚£è€…æ•°æ®ï¼Œç§»é™¤äº†ç¼ºå¤±å…³é”®å˜é‡çš„æ ·æœ¬ã€‚

### 3. real_confounder_detailed_analysis.csv
è¯¦ç»†çš„æ··æ‚å› ç´ åˆ†æï¼ŒåŒ…å«æ‰€æœ‰ç”Ÿç‰©æ ‡å¿—ç‰©ç»„åˆçš„ç›¸å…³æ€§ã€‚

### 4. real_causal_adjustment_benefits.csv
æ¯ä¸ªæ··æ‚å› ç´ çš„å› æœè°ƒæ•´æ”¶ç›Šè¯¦ç»†åˆ†æã€‚

## ç»˜å›¾è¯´æ˜
ä½¿ç”¨ Real_Figure1E_plot_data.csv å¯ä»¥åœ¨ä»»ä½•è½¯ä»¶ä¸­é‡ç°Figure 1Eï¼š

**å›¾è¡¨ç±»å‹**: åŒYè½´æŸ±çŠ¶å›¾
**Xè½´**: Confounder (æ··æ‚å› ç´ åç§°)
**å·¦Yè½´**: Confounding_Strength (æ©™è‰²ï¼Œæ··æ‚å¼ºåº¦)
**å³Yè½´**: Causal_Adjustment_Benefit (è“ç»¿è‰²ï¼Œé¢„æµ‹æ”¹å–„)

**å»ºè®®é¢œè‰²**:
- å·¦æŸ±: #FF9F40 (æ©™è‰²)
- å³æŸ±: #4BC0C0 (è“ç»¿è‰²)

## ä¸»è¦å‘ç°
{figure1e_data['Confounder'].iloc[0] if not figure1e_data.empty else 'BMI'} æ˜¯æœ€å¼ºçš„æ··æ‚å› ç´  (å¼ºåº¦: {figure1e_data['Confounding_Strength'].iloc[0]:.3f if not figure1e_data.empty else 'N/A'})
"""
    
    with open('real_data_description.txt', 'w', encoding='utf-8') as f:
        f.write(data_description)
    print("çœŸå®æ•°æ®è¯´æ˜æ–‡ä»¶å·²ä¿å­˜è‡³: real_data_description.txt")

def main():
    """
    ä¸»å‡½æ•°ï¼šä½¿ç”¨çœŸå®æ•°æ®ç”ŸæˆFigure 1E
    """
    print("=== åŸºäºçœŸå®æ•°æ®çš„ Figure 1E ç”Ÿæˆå™¨ ===")
    print("æ­£åœ¨åˆ†ææ‚¨çš„çœŸå®æ‚£è€…æ•°æ®...\n")
    
    # åŠ è½½çœŸå®æ•°æ®
    df = load_real_data()
    if df is None:
        return
    
    # æ¸…æ´—å’Œå‡†å¤‡æ•°æ®
    df_clean = clean_and_prepare_data(df)
    if df_clean.empty:
        print("é”™è¯¯: æ¸…æ´—åæ²¡æœ‰å¯ç”¨æ•°æ®")
        return
    
    # æ•°æ®æ¦‚è§ˆ
    print(f"\n=== æ•°æ®æ¦‚è§ˆ ===")
    print(f"æ‚£è€…æ•°é‡: {df_clean['PatientID'].nunique()}")
    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(df_clean)}")
    print(f"å¯ç”¨åˆ—: {list(df_clean.columns)}")
    
    # åŸºç¡€ç»Ÿè®¡
    if 'SweatCH' in df_clean.columns and 'BloodCH' in df_clean.columns:
        basic_corr = df_clean['SweatCH'].corr(df_clean['BloodCH'])
        print(f"æ±—æ¶²-è¡€æ¶²èƒ†å›ºé†‡åŸºç¡€ç›¸å…³æ€§: {basic_corr:.3f}")
    
    # ç”Ÿæˆåˆ†ææ•°æ®
    confounder_analysis, adjustment_benefits, figure1e_data = create_real_figure1e_data(df_clean)
    
    if figure1e_data is None or figure1e_data.empty:
        print("é”™è¯¯: æ— æ³•ç”ŸæˆFigure 1Eæ•°æ®")
        return
    
    # ä¿å­˜æ‰€æœ‰æ•°æ®
    print("\n=== ä¿å­˜åˆ†æç»“æœ ===")
    save_real_data_csv(df_clean, confounder_analysis, adjustment_benefits, figure1e_data)
    
    # åˆ›å»ºå›¾è¡¨
    print("\n=== ç”Ÿæˆå›¾è¡¨ ===")
    try:
        plot_real_figure1e_matplotlib(figure1e_data)
    except Exception as e:
        print(f"matplotlibç»˜å›¾å‡ºé”™: {e}")
    
    try:
        plot_real_figure1e_plotly(figure1e_data)
    except Exception as e:
        print(f"plotlyç»˜å›¾å‡ºé”™: {e}")
    
    print("\n=== å®Œæˆï¼===")
    print("ğŸ¯ æ‚¨çš„åŒäº‹ç°åœ¨å¯ä»¥ä½¿ç”¨ 'Real_Figure1E_plot_data.csv' é‡ç°Figure 1E")
    print("ğŸ“Š æ¨èç»˜å›¾è®¾ç½®ï¼š")
    print("   - Xè½´: Confounder")
    print("   - å·¦Yè½´ (æ©™è‰²): Confounding_Strength") 
    print("   - å³Yè½´ (è“ç»¿è‰²): Causal_Adjustment_Benefit")
    print("   - æ•°æ®å·²æŒ‰æ··æ‚å¼ºåº¦é™åºæ’åˆ—")

if __name__ == "__main__":
    main()